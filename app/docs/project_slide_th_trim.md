# โครงงานระบบสมาร์ทโฮม — สไลด์ (เฉพาะหัวข้อที่กำหนด)

---

## ที่มาและความสำคัญของโครงงาน
- บ้าน/ฟาร์ม/ออฟฟิศขนาดเล็กมักไม่มีระบบรวมศูนย์ในการดูสถานะและควบคุมอุปกรณ์ ทำให้ตรวจยากและใช้เวลามาก
- อุปกรณ์ IoT หลายยี่ห้อทำให้ผู้ใช้ต้องสลับหลายแอป โครงงานนี้รวม “แสดงผล-ควบคุม-สั่งงานด้วยเสียง-อัตโนมัติ” ไว้ในเว็บเดียว ใช้งานได้ข้ามอุปกรณ์
- ใช้ AI วิเคราะห์ภาษามนุษย์ ช่วยให้สั่งงานได้ตรงความต้องการ โดยไม่ต้องคลิกเมนูซับซ้อน และเปิดทางสู่ผู้ช่วยบ้านอัจฉริยะ

---

## การนำไปใช้ประโยชน์
- บ้านอัจฉริยะ: ควบคุมไฟ พัดลม แอร์ ปั๊มน้ำ ฮีทเตอร์ และดูค่าความร้อน/ความชื้น/แก๊สแบบเรียลไทม์
- ฟาร์ม/เรือนเพาะ: รดน้ำ/พ่นหมอกอัตโนมัติ แจ้งเตือนค่าเซนเซอร์ผิดปกติ พร้อมบันทึกประวัติ
- ห้องเซิร์ฟเวอร์/คลังสินค้า: เฝ้าระวังอุณหภูมิ/ความชื้น เก็บประวัติสำหรับตรวจสอบย้อนหลัง
- สถานศึกษา/สาธิต: กรณีศึกษา IoT + AI ครบวงจรสำหรับการเรียนการสอนและวิจัย

---

## ขอบเขตของโครงงาน
- ขอบเขตที่ทำแล้ว
  - แดชบอร์ด Flutter Web แสดงสถานะและกราฟเซนเซอร์แบบเรียลไทม์
  - ควบคุมอุปกรณ์ผ่าน API และ/หรือ MQTT (เลือกโหมดได้/โหมดอัตโนมัติลอง API ก่อน ค่อยสลับ MQTT)
  - คำสั่งเสียง (Speech-to-Text) + วิเคราะห์เจตนาด้วย AI + ตอบกลับด้วยเสียง (TTS ผ่านพร็อกซีฝั่งเซิร์ฟเวอร์)
  - เก็บค่าตั้งค่า/แคชข้อมูลบนเครื่องผู้ใช้ (SharedPreferences)
  - ออโต้รีเฟรช/อัปเดตเรียลไทม์ และ Automation พื้นฐาน
- นอกขอบเขต (ทำต่อได้ในอนาคต)
  - ระบบยืนยันตัวตน/บทบาท (Auth/RBAC) และ HTTPS
  - แจ้งเตือนนอกเครือข่าย (Push/Email/SMS)
  - แอปมือถือ Native และ Edge Computing

---

## การออกแบบระบบเบื้องต้น
- แยก Frontend (Web :8088) ออกจาก Backend (API :8080) เพื่อความปลอดภัย/ยืดหยุ่น
- Backend ทำหน้าที่พร็อกซีไป OpenAI (ซ่อน API key) และจัดการ CORS/ความปลอดภัย
- ใช้ MQTT สำหรับอัปเดตสถานะเรียลไทม์/ควบคุมแบบหน่วงต่ำ

```mermaid
flowchart LR
  subgraph Client (PC/iPad/Mobile Browser)
    UI[Flutter Web]
  end
  UI -- REST --> API[(Backend :8080)]
  UI -- WebSocket --> MQTT[(MQTT Broker)]
  API -- HTTPS --> OpenAI[(OpenAI Chat/TTS)]
```

โค้ดที่เกี่ยวข้องหลัก
- `lib/screens/dashboard_screen.dart` หน้าควบคุมและแสดงผลรวม
- `lib/screens/chat_screen.dart` หน้า Log/แชทกับ AI และปุ่มเล่นเสียง
- `lib/services/voice_command_service.dart` ท่อคำสั่งเสียง (ฟัง-ตีความ-พูดตอบ-สั่งอุปกรณ์)
- `lib/services/ai_service.dart` วิเคราะห์คำสั่งด้วย OpenAI หรือ Local AI
- `lib/services/tts_service.dart` เล่นเสียงตอบกลับผ่าน `/api/tts`
- `lib/utils/constants.dart` กำหนด endpoints และค่าคงที่ของระบบ

---

## คลาวด์และเว็บเซอร์วิสที่ใช้
- OpenAI: ใช้สำหรับ Chat/Intent และ TTS (เรียกจาก Backend เท่านั้น)
- MQTT Broker: ตัวอย่างค่าเริ่มต้น HiveMQ (WebSocket) สำหรับการเดโมบนเว็บ
- ตัวเลือกคลาวด์/โฮสต์ (ใช้ได้ในอนาคต)
  - โฮสต์ Backend บน Azure/AWS/GCP พร้อม HTTPS/โดเมนจริง
  - ใช้ Cloud tunneling (ngrok/cloudflared) เพื่อทดสอบ HTTPS ชั่วคราว
  - จัดเก็บประวัติ/กฎอัตโนมัติในฐานข้อมูลคลาวด์ (PostgreSQL/Firestore)

---

## การควบคุมและแสดงผล
- การแสดงผล
  - แดชบอร์ดสถานะสด กราฟเซนเซอร์แบบอนุกรมเวลา และ Log เหตุการณ์
  - อัปเดตอัตโนมัติ/ฟัง MQTT เพื่อความหน่วงต่ำ
- การควบคุม
  - ผ่าน API: `POST /api/control` และ endpoints อื่นๆ (เช่น `/api/status`, `/api/sensors`)
  - ผ่าน MQTT: ส่งคำสั่งไปยัง topic เช่น `home/command`
  - โหมดอัตโนมัติ: ลอง API ก่อน ถ้าไม่สำเร็จสลับไป MQTT
- TTS/Voice UX
  - กดปุ่มลำโพงเพื่อเล่นเสียงคำตอบของ AI (เหมาะกับ iOS ที่ต้องการ user gesture)
